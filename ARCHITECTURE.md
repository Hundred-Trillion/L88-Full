# L88 â€” Architecture Reference

L88 is a local-first agentic RAG system built for scientific research. This document covers the full technical architecture: ingestion pipeline, retrieval system, graph orchestration, and frontend design.

---

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Frontend (React + Vite)                             â”‚
â”‚                                                                              â”‚
â”‚  LoginPage   â†’   Sidebar (Sessions)   â”‚   ChatPanel           â”‚  RightPanel â”‚
â”‚                  Session CRUD         â”‚   Messages            â”‚  Documents  â”‚
â”‚                  Role Switcher        â”‚   PipelineStatus HUD  â”‚  Scratchpad â”‚
â”‚                                       â”‚   Stop Generation     â”‚  Members    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          Backend (FastAPI + SQLModel)                        â”‚
â”‚                                                                              â”‚
â”‚  /auth  â”‚  /sessions  â”‚  /documents  â”‚  /members  â”‚  /scratchpad  â”‚  /admin â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Intelligence Layer (LangGraph + Ollama)                  â”‚
â”‚                                                                              â”‚
â”‚   Router â”€â”€â–º Analyzer â”€â”€â–º Rewriter â”€â”€â–º Retrieval â”€â”€â–º Generator              â”‚
â”‚                                â–²                          â”‚                  â”‚
â”‚                                â””â”€â”€â”€â”€â”€â”€ Self-Evaluator â—„â”€â”€â”€â”˜                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          Storage                                             â”‚
â”‚                                                                              â”‚
â”‚   SQLite (l88.db)   â”‚   FAISS index (disk)   â”‚   BM25 index (disk)         â”‚
â”‚   Sessions, Users,  â”‚   Per-session vectors  â”‚   Per-session keyword index  â”‚
â”‚   Documents, Msgs   â”‚   + Library index      â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ§  Agentic RAG Pipeline

The core logic is a LangGraph `StateGraph`. Every user query traverses this graph.

```mermaid
graph TD
    Q[User Query] --> Router{Router}

    Router -- "no docs / general" --> DirectGen[Direct Generator]
    Router -- "summarize" --> Summarizer[Summarizer]
    Router -- "rag" --> Analyzer[Query Analyzer]

    Analyzer --> Rewriter[Query Rewriter\nacronym expansion\nmulti-query generation]
    Rewriter --> Retrieval[Retrieval Node\nFAISS + BM25 hybrid\nscore normalization\ncross-encoder rerank]

    Retrieval --> Generator[Generator\nstructured JSON\nreasoning trace\ncitations]
    Generator --> Evaluator[Self-Evaluator\nconfidence threshold]

    Evaluator -- "confident â‰¥ 0.7" --> Output[Final Answer]
    Evaluator -- "gap found" --> Rewriter
    DirectGen --> Output
    Summarizer --> Output
```

### Graph Nodes

| Node | Description |
|---|---|
| **Router** | Checks session type, doc selection, summarization triggers. Pure logic, no LLM. |
| **Query Analyzer** | Classifies query: `simple`, `multi_hop`, `math`, `comparison`. Determines retrieval strategy. |
| **Query Rewriter** | Expands acronyms (`TRL` â†’ `Technology Readiness Level`), generates 1â€“3 rewritten sub-queries for broader retrieval coverage. Always includes the original query. |
| **Retrieval** | Embeds queries with BGE prefix, searches FAISS + BM25, normalizes scores to [0,1], blends with adaptive weights, cross-encoder reranks to top-N. |
| **Generator** | Single structured LLM call producing `answer`, `reasoning`, `sources`, `context_verdict`, `missing_info`. |
| **Self-Evaluator** | Checks reranker confidence score. If `< 0.7` and retries remain, sends back to Rewriter with a verdict hint. |

---

## ðŸ“„ Ingestion Pipeline

When a PDF is uploaded:

```
PDF file
  â””â”€â”€ parser.py (PyMuPDF)
        â”œâ”€â”€ Extract raw text page-by-page
        â”œâ”€â”€ Detect cross-page repeating lines â†’ strip headers/footers/page numbers
        â””â”€â”€ Return cleaned page texts
              â””â”€â”€ chunker.py
                    â”œâ”€â”€ pysbd sentence segmentation (handles "Fig. 3", "et al.", etc.)
                    â”œâ”€â”€ RecursiveCharacterTextSplitter (380 tokens, 45 overlap)
                    â””â”€â”€ tiktoken cl100k_base token counting
                          â””â”€â”€ embedder.py
                                â”œâ”€â”€ BAAI/bge-base-en-v1.5 (CPU)
                                â”œâ”€â”€ BGE retrieval prefix applied
                                â””â”€â”€ L2-normalized â†’ cosine via FAISS IP
                                      â”œâ”€â”€ vectorstore.py (FAISS IndexFlatIP)
                                      â””â”€â”€ bm25store.py (BM25Okapi)
```

**Parser hardening**: Two-pass approach. Pass 1 extracts all pages. Common lines appearing on â‰¥2 pages (headers, footers, "Page X of Y", watermarks) are collected. Pass 2 strips those lines and bare page-number patterns before chunking. This prevents boilerplate from polluting chunk relevance scores.

---

## ðŸ” Retrieval System

### Hybrid FAISS + BM25

For each rewritten query (up to 3):

1. **FAISS search** â€” Top-20 by cosine similarity (via `IndexFlatIP` on L2-normalized vectors)
2. **BM25 search** â€” Top-20 by BM25Okapi score with stopword-filtered tokenizer
3. **Score normalization** â€” BM25 raw scores (0 â†’ âˆž) are min-max normalized to [0,1] so they're commensurable with FAISS cosine scores
4. **Adaptive weight blend**:
   - `simple` queries: 40% FAISS + 60% BM25 (favour exact match)
   - `multi_hop/math/comparison`: 80% FAISS + 20% BM25 (favour semantics)
   - Falls back to 100% of whichever index has results
5. **Deduplication** by `(doc_id, chunk_idx)` across all queries
6. **Document filter** by user-selected doc IDs
7. **Cross-encoder rerank** â€” `bge-reranker-v2-m3` scores all candidates, returns top-7

### BM25 Tokenizer

Unlike the naive `text.split()`, the improved tokenizer:
- Splits on all punctuation (colons, brackets, quotes, etc.) but preserves hyphens
- Removes ~50 common English stopwords
- Result: `"TRL level"` shares tokens with `"Technology Readiness Level (TRL)"` correctly

### Models

| Model | Task | Device | Dim |
|---|---|---|---|
| `BAAI/bge-base-en-v1.5` | Embedding | CPU | 768 |
| `BAAI/bge-reranker-v2-m3` | Cross-encoder reranking | CPU | â€” |
| `qwen2.5-7b-awq` | Text generation | GPU | â€” |

---

## ðŸ”¤ Query Rewriting & Acronym Expansion

Before the LLM call, `_pre_expand_acronyms()` detects ALL-CAPS tokens (e.g. `TRL`, `SLA`) and appends them as expansion hints to the prompt. The LLM is then explicitly instructed via system prompt to:

1. Expand all abbreviations and acronyms
2. Include both the short form and full form in every query
3. For "what is X" queries, also generate `"X definition"` and `"X explained"` variants

The original raw query is always included in the final candidate set (belt + suspenders for exact-term matching).

---

## ðŸ—ƒ Database Schema

| Table | Key Fields |
|---|---|
| `user` | `id`, `username`, `password_hash`, `role`, `active` |
| `session` | `id`, `name`, `owner_id`, `type` (general/rag), `web_mode` |
| `sessionmember` | `session_id`, `user_id`, `role` |
| `document` | `id`, `session_id`, `filename`, `source`, `page_count`, `chunk_count`, `selected` |
| `message` | `id`, `session_id`, `user_id`, `role`, `content`, `reasoning`, `confident`, `context_verdict`, `missing_info`, `retrieval_debug`, `created_at` |
| `scratchpad` | `session_id`, `content` |

---

## âš¡ Async Document Deletion

Deleting a document requires rebuilding the FAISS + BM25 index without the deleted doc's chunks. This is slow (re-embeds all remaining docs). The router now:

1. Removes the DB record and PDF file **synchronously**
2. Returns `200 {"detail": "Document deleted"}` immediately
3. Runs `_rebuild_session_index()` via FastAPI `BackgroundTasks`

This prevents HTTP timeouts on large document sets.

---

## ðŸŽ¨ Frontend

| Component | Responsibility |
|---|---|
| `App.tsx` | Root layout, session state, AbortController for stop generation, deletingDocs tracking |
| `Sidebar.tsx` | Session list, create/delete session, role switcher |
| `ChatPanel.tsx` | Message list, input, stop button, `PipelineStatus` ambient HUD |
| `RightPanel.tsx` | Document list (with async delete spinner), scratchpad, member management |
| `LoginPage.tsx` | JWT auth form |
| `api.ts` | HTTP client with AbortSignal support for cancelling in-flight requests |

### PipelineStatus HUD

While the backend is processing, the loading dots are replaced by a cycling monospaced status line:

```
analyzing query...
rewriting queries..
searching vector store.
reranking evidence...
generating answer..
```

Each step fades in/out with a subtle slide animation via Framer Motion.

### Stop Generation

An `AbortController` is created per request in `handleSend`. When the stop button is clicked, `controller.abort()` cancels the fetch. The backend receives the disconnection and stops streaming.

---

## ðŸ”’ Security & Roles

- **Auth**: JWT (HS256), 8-hour expiry
- **RBAC**: Global role per user (`admin` / `chat` / `read_only`) + optional per-session override via `SessionMember`
- **Role resolution**: Session membership role takes precedence over global role for that session

| Capability | Admin | Chat | Read Only |
|---|:---:|:---:|:---:|
| Send messages | âœ… | âœ… | âŒ |
| Upload / delete documents | âœ… | âŒ | âŒ |
| Toggle document selection | âœ… | âœ… | âŒ |
| Manage members | âœ… | âŒ | âŒ |
| Edit scratchpad | âœ… | âŒ | âŒ |
| View everything | âœ… | âœ… | âœ… |

---

## âš™ï¸ Configuration (`config.py`)

| Parameter | Default | Description |
|---|---|---|
| `CHUNK_SIZE` | 380 | Tokens per chunk |
| `CHUNK_OVERLAP` | 45 | Token overlap |
| `RETRIEVE_TOP_K` | 20 | Candidates per query (FAISS + BM25 each) |
| `RERANK_TOP_N` | 7 | Final chunks after reranking |
| `MAX_ALT_QUERIES` | 3 | Rewritten queries per attempt |
| `MAX_REWRITES` | 2 | Max retry loops |
| `LLM_NUM_CTX` | 16384 | Context window (overrides Ollama default of 4096) |
| `LLM_TEMPERATURE` | 0 | Deterministic generation |
| `EMBED_MODEL` | `bge-base-en-v1.5` | Embedding model |
| `RERANKER_MODEL` | `bge-reranker-v2-m3` | Reranker model |
| `LLM_MODEL` | `qwen2.5-7b-awq` | Primary LLM (GPU) |
| `LLM_MODEL_FALLBACK` | `qwen2.5:14b` | CPU fallback |
